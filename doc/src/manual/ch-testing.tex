\chapter{Testing}
\label{cha:testing}

\section{Overview}
\label{sec:testing:overview}

\subsection{Verification, Validation}
\label{sec:testing:verification-validation}

Correctness of the simulation model is a primary concern of the developers
and users of the model, because they want to obtain credible simulation
results. Verification and validation are activities conducted during the
development of a simulation model with the ultimate goal of producing an
accurate and credible model.

\begin{itemize}
\item \textbf{Verification} of a model is the process of confirming that it is
    correctly implemented with respect to the conceptual model, that is, it
    matches specifications and assumptions deemed acceptable for the given
    purpose of application. During verification, the model is tested to find
    and fix errors in the implementation of the model.
\item \textbf{Validation} checks the accuracy of the model's representation of
    the real system. Model validation is defined to mean ``substantiation that
    a computerized model within its domain of applicability possesses a
    satisfactory range of accuracy consistent with the intended application of
    the model''. A model should be built for a specific purpose or set of
    objectives and its validity determined for that purpose.
\end{itemize}

Of the two, verification is essentially a software engineering issue, so it
can be assisted with tools used for software quality assurance, for example
testing tools. Validation is not a software engineering issue.

\subsection{Unit Testing, Regression Testing}
\label{sec:testing:unit-testing-regression-testing}

As mentioned above, software testing techniques can be of significant help
during model verification. Testing can also help to ensure that
a simulation model that once passed validation and verification will also
remain correct for an extended period.

Software testing is an art on its own, with several techniques and
methodologies. Here we'll only mention two types that are important for us,
regression testing and unit testing.

\begin{itemize}
\item \textbf{Regression testing} is a technique that seeks to uncover new
    software bugs, or regressions, in existing areas of a system after changes
    such as enhancements, patches or configuration changes, have been made to
    them.
\item \textbf{Unit testing} is a method by which individual units of
    source code are tested to determine if they are fit for use. In an
    object-oriented environment, this is usually done at the class level.
\end{itemize}

The two may overlap; for example, unit tests are also useful for discovering
regressions.

One way of performing regression testing on an {\opp} simulation model is
to record the log produced during simulation, and compare it to a
pre-recorded log. The drawback is that code refactoring may nontrivially
change the log as well, making it impossible to compare to the pre-recorded
one. Alternatively, one may just compare the result files or only certain
simulation results and be free of the refactoring effects, but then certain
regressions may escape the testing. This type of tradeoff seems to be
typical for regression testing.

Unit testing of simulation models may be done on class level or module
level. There are many open-source unit testing frameworks for C++, for
example CppUnit, Boost Test, Google Test, UnitTest++, just to name a few.
They are well suited for class-level testing. However, they are usually
cumbersome to apply to testing modules due to the peculiarities of the
domain (network simulation) and {\opp}.

A test in an \textit{xUnit}-type testing framework (a collective name for
CppUnit-style frameworks) operates with various assertions to test
function return values and object states. This approach is difficult to
apply to the testing of {\opp} modules that often operate in a complex
environment (cannot be easily instantiated and operated in isolation),
react to various events (messages, packets, signals, etc.), and have
complex dynamic behavior and substantial internal state.

Later sections will introduce \fprog{opp\_test}, a tool {\opp} provides
for assisting various testing task; and summarize various testing methods
useful for testing simulation models.


\section{The opp\_test Tool}
\label{sec:testing:opptest}

\subsection{Introduction}
\label{sec:testing:opptest:introduction}

This section documents the \fprog{opp\_test}, a versatile tool that is
helpful for various testing scenarios. \fprog{opp\_test} can be used for
various types of tests, including unit tests and regression tests. It was
originally written for testing the {\opp} simulation kernel, but it is
equally suited for testing functions, classes, modules, and whole
simulations.

\fprog{opp\_test} is built around a simple concept: it lets one define
simulations in a concise way, runs them, and checks that the output (result
files, log, etc.) matches a predefined pattern or patterns. In many cases,
this approach works better than inserting various assertions into the code
(which is still also an option).

Each test is a single file, with the \ttt{.test} file extension. All NED
code, C++ code, ini files and other data necessary to run the test case as
well as the PASS criteria are packed together in the test file. Such
self-contained tests are easier to handle, and also encourage authors to
write tests that are compact and to the point.

Let us see a small test file, \ttt{cMessage\_properties\_1.test}:

\begin{filelisting}
%description:
Test the name and length properties of cPacket.

%activity:
cPacket *pk = new cPacket();
pk->setName("ACK");
pk->setByteLength(64);
EV << "name: " << pk->getName() << endl;
EV << "length: " << pk->getByteLength() << endl;
delete pk;

%contains: stdout
name: ACK
length: 64
\end{filelisting}

What this test says is this: create a simulation with a simple module
that has the above C++ code block as the body of the \ttt{activity()} method,
and when run, it should print the text after the \ttt{\%contains} line.

To run this test, we need a \textit{control script}, for example
\ttt{runtest} from the \ttt{omnetpp/test/core} directory. \ttt{runtest}
itself relies on the \fprog{opp\_test} tool.

\begin{note}
The control script is not part of {\opp} because it is somewhat specific to
the simulation model or framework being tested, but it is usually trivial
to write. A later section will explain how write the control script.
\end{note}

The output will be similar to this one:

\begin{filelisting}
$ ./runtest cMessage_properties_1.test
opp_test: extracting files from *.test files into work...
Creating Makefile in omnetpp/test/core/work...
cMessage_properties_1/test.cc
Creating executable: out/gcc-debug/work
opp_test: running tests using work.exe...
*** cMessage_properties_1.test: PASS
========================================
PASS: 1   FAIL: 0   UNRESOLVED: 0

Results can be found in work/
\end{filelisting}

This was a passing test. What would constitute a fail?

\begin{itemize}
\item crash
\item simulation runtime error
\item nonzero exit code (a simulation runtime error is also detected by nonzero exit code)
\item the output doesn't match the expectation (there are several possibilities
   for expressing what is expected: multiple match criteria, literal string vs regex,
   positive vs negative match, matching against the standard output, standard error
   or any file, etc.)
\end{itemize}

One normally wants to run several tests together. The \ttt{runtest} script accepts
several \ttt{.test} files on the command line, and when started without
arguments, it defaults to \ttt{*.test}, all test files in the current
directory. At the end of the run, the tool prints summary statistics
(number of tests passed, failed, and being unresolved).

An example run from \ttt{omnetpp/test/core} (some lines were removed from
the output, and one test was changed to show a failure):

\begin{filelisting}
$ ./runtest cSimpleModule-*.test
opp_test: extracting files from *.test files into work...
Creating Makefile in omnetpp/test/core/work...
[...]
Creating executable: out/gcc-debug/work
opp_test: running tests using work...
*** cSimpleModule_activity_1.test: PASS
*** cSimpleModule_activity_2.test: PASS
[...]
*** cSimpleModule_handleMessage_2.test: PASS
*** cSimpleModule_initialize_1.test: PASS
*** cSimpleModule_multistageinit_1.test: PASS
*** cSimpleModule_ownershiptransfer_1.test: PASS
*** cSimpleModule_recordScalar_1.test: PASS
*** cSimpleModule_recordScalar_2.test: FAIL (test-1.sca fails %contains-regex(2) rule)
expected pattern:
>>>>run General-1-.*?
scalar Test 	one 	24.2
scalar Test 	two 	-1.5888<<<<
actual output:
>>>>version 2
run General-1-20141020-11:39:34-1200
attr configname General
attr datetime 20141020-11:39:34
attr experiment General
attr inifile _defaults.ini
[...]
scalar Test 	one 	24.2
scalar Test 	two 	-1.5
<<<<
*** cSimpleModule_recordScalar_3.test: PASS
*** cSimpleModule_scheduleAt_notowner_1.test: PASS
*** cSimpleModule_scheduleAt_notowner_2.test: PASS
[...]
========================================
PASS: 36   FAIL: 1   UNRESOLVED: 0
FAILED tests: cSimpleModule_recordScalar_2.test

Results can be found in work/
\end{filelisting}

Note that code from all tests were linked to form a single executable, which saves
time and disk space compared to per-test executables or libraries.

A test file like the one above is useful for unit testing of classes or functions.
However, as we will see, the test framework provides further facilities that make
it convenient for testing modules and whole simulations as well.

%% But: can link to external code; can load external model code (NED, C++);
%% common parts can be factored out; pre- and postprocessing

The following sections go into details about the syntax and features of \ttt{.test} files,
about writing the control script, and give advice on how to cover several use
cases with the \fprog{opp\_test} tool.


\subsection{Terminology}
\label{sec:testing:opptest:terminology}

The next sections will use the following language:

\begin{itemize}
\item \textit{test file}: A file with the \ttt{.test} extension that \fprog{opp\_test} understands.
\item \textit{test tool}: The \fprog{opp\_test} program
\item \textit{control script}: A script that relies on \fprog{opp\_test} to run the tests.
  The control script is not part of {\opp} because it usually needs to be somewhat
  specific to the simulation model or framework being tested.
\item \textit{test program}: The simulation program whose output is checked by the test.
  It is usually \ttt{work/work} (\ttt{work/work.exe} on Windows). However, it is
  also possible to let the control script build a dynamic library from the test code, and
  then use e.g. \fprog{opp\_run} as test program.
\item \textit{test directory}: The directory where a \ttt{.test} file
  is extracted; usually \ttt{work/<testname>/}. It is also set as working
  directory for running the test program.
\end{itemize}


\subsection{Test File Syntax}
\label{sec:testing:opptest:test-file-syntax}

Test files are composed of \%-directives of the syntax:

\begin{filelisting}
%<directive>: <value>
<body>
\end{filelisting}

The body extends up to the next directive (the next line starting with \%),
or to the end of the file. Some directives require a value, others a body, or both.

Certain directives, e.g. \ftest{\%contains}, may occur several times in the file.

\subsection{Test Description}
\label{sec:testing:opptest:test-description}

Syntax:
\begin{filelisting}
%description:
<test-description-lines>
\end{filelisting}

\ftest{\%description} is customarily written at the top of the \ttt{.test}
file, and lets one provide a multi-line comment about the purpose of the
test. It is recommended to invest time into well-written descriptions,
because they make determining the original purpose of a test that has
become broken significantly easier.

\subsection{Test Code Generation}
\label{sec:testing:opptest:test-code-generation}

This section describes the directives used for creating C++ source and
other files in the test directory.

\subsubsection{\%activity}
\label{sec:testing:opptest:activity}

Syntax:

\begin{filelisting}
%activity:
<body-of-activity()>
\end{filelisting}

\ftest{\%activity} lets one write test code without much boilerplate. The
directive generates a simple module that contains a single \ttt{activity()}
method with the given code as method body.

A NED file containing the simple module's (barebones) declaration, and an
ini file to set up the module as a network are also generated.


\subsubsection{\%module}
\label{sec:testing:opptest:module}

Syntax:

\begin{filelisting}
%module: <modulename>
<simple-module-C++-definition>
\end{filelisting}

\ftest{\%module} lets one define a module class and run it as the only module
in the simulation.

A NED file containing the simple module's (barebones) declaration, and an
ini file to set up the module as a network are also generated.


\subsubsection{\%includes, \%global}
\label{sec:testing:opptest:includes-and-global}

Syntax:

\begin{filelisting}
%includes:
<#include directives>
\end{filelisting}

\begin{filelisting}
%global:
<global-code-pasted-before-activity>
\end{filelisting}

\ftest{\%includes} and \ftest{\%global} are helpers for \ftest{\%activity}
and \ftest{\%module}, and let one insert additional lines into the
generated C++ code.

Both directives insert the code block above the module C++ declaration. The only
difference is in their relation to the C++ namespace: the body of \ftest{\%includes}
is inserted above (i.e. outside) the namespace, and the body of \ftest{\%globals}
is inserted inside the namespace.


\subsubsection{The Default Ini File}
\label{sec:testing:opptest:default-ini-file}

The following ini file is always generated:

\begin{inifile}
[General]
network = <network-name>
cmdenv-express-mode = false
\end{inifile}

The network name in the file is chosen to match the module
generated with \ftest{\%activity} or \ftest{\%module}; if they
are absent, it will be \ttt{Test}.

\subsubsection{\%network}
\label{sec:testing:opptest:network}

Syntax:

\begin{filelisting}
%network: <network-name>
\end{filelisting}

This directive can be used to override the network name in the default ini file.


\subsubsection{\%file, \%inifile}
\label{sec:testing:opptest:file-and-inifile}

Syntax:

\begin{filelisting}
%file: <file-name>
<file-contents>
\end{filelisting}

\begin{filelisting}
%inifile: [<inifile-name>]
<inifile-contents>
\end{filelisting}

\ftest{\%file} saves a file with the given file name and content into the test's
extraction folder in the preparation phase of the test run. It is customarily
used for creating NED files, MSG files, ini files, and extra data files
required by the test. There can be several \ftest{\%file} sections in the test file.

\ftest{\%inifile} is similar to \ftest{\%file} in that it also saves a file with the
given file name and content, but it additionally also adds the file to the simulation's
command line, causing the simulation to read it as an (extra) ini file.
There can be several \ftest{\%inifile} sections in the test file.

The default ini file is always generated.


\subsubsection{The @TESTNAME@ Macro}
\label{sec:testing:opptest:testname-macro}

In test files, the string \ttt{@TESTNAME@} will be replaced with the test
case name. Since it is substituted everywhere (C++, NED, msg and ini
files), one can also write things like \ttt{@TESTNAME@\_function()}, or
\ttt{printf("this is @TESTNAME@{\textbackslash}n")}.

\subsubsection{Avoiding C++ Name Clashes}
\label{sec:testing:opptest:avoiding-cpp-name-clashes}

Since all sources are compiled into a single test executable, actions have
to be taken to prevent accidental name clashes between C++ symbols in
different test cases. A good way to ensure this is place all code into
namespaces named after the test cases.

\begin{filelisting}
namespace @TESTNAME@ {
   ...
};
\end{filelisting}

This is done automatically for the \ftest{\%activity}, \ftest{\%module},
\ftest{\%global} blocks, but for other files (e.g. source files generated
via \ftest{\%file}, that needs to be done manually.


\subsection{PASS Criteria}
\label{sec:testing:opptest:pass-criteria}

\subsubsection{\%contains, \%contains-regex, \%not-contains, \%not-contains-regex}
\label{sec:testing:opptest:contains-and-co}

Syntax:

\begin{filelisting}
%contains: <output-file-to-check>
<multi-line-text>
\end{filelisting}

\begin{filelisting}
%contains-regex: <output-file-to-check>
<multi-line-regexp>
\end{filelisting}

\begin{filelisting}
%not-contains: <output-file-to-check>
<multi-line-text>
\end{filelisting}

\begin{filelisting}
%not-contains-regex: <output-file-to-check>
<multi-line-regexp>
\end{filelisting}

These directives let one check for the presence (or absence) of certain text in
the output. One can check a file, or the standard output or standard error of
the test program; for the latter two, \ttt{stdout} and \ttt{stderr} needs to be
specified as file name, respectively. If the file is not found, the test will be marked
as \textit{error}. There can be several \ftest{\%contains}-style directives
in the test file.

The text or regular expression can be multi-line. Before match is attempted,
trailing spaces are removed from all lines in both the pattern and the
file contents; leading and trailing blank lines in the patterns are removed;
and any substitutions are performed (see \ftest{\%subst}). Perl-style regular
expressions are accepted.

To facilitate debugging of tests, the text/regex blocks are saved into
the test directory.


\subsubsection{\%subst}
\label{sec:testing:opptest:subst}

Syntax:

\begin{filelisting}
%subst: /<search-regex>/<replacement>/<flags>
\end{filelisting}

It is possible to apply text substitutions to the output before it is
matched against expected output. This is done with \ftest{\%subst}
directive; there can be more than one \ftest{\%subst} in a test file. It
takes a Perl-style regular expression to search for, a replacement text,
and flags, in the \textit{/search/replace/flags} syntax. Flags can be empty
or a combination of the letters \ttt{i}, \ttt{m}, and \ttt{s}, for
case-insensitive, multi-line or single-string match (see the Perl regex
documentation.)

\ftest{\%subst} was primarily invented to deal with differences in printf
output across platforms and compilers: different compilers print infinite
and not-a-number in different ways: \ttt{1.\#INF}, \ttt{inf}, \ttt{Inf},
\ttt{-1.\#IND}, \ttt{nan}, \ttt{NaN} etc. With \ftest{\%subst}, they can be
brought to a common form:

\begin{filelisting}
%subst: /-?1\.#INF/inf/
%subst: /-?1\.#IND/nan/
%subst: /-?1\.#QNAN/nan/
%subst: /-?NaN/nan/
%subst: /-?nan/nan/
\end{filelisting}

\subsubsection{\%exitcode, \%ignore-exitcode}
\label{sec:testing:opptest:exitcode}

Syntax:
\begin{filelisting}
%exitcode: <one-or-more-numeric-exit-codes>
\end{filelisting}

\begin{filelisting}
%ignore-exitcode: 1
\end{filelisting}

\ftest{\%exitcode} and \ftest{\%ignore-exitcode} let one test the exit code of the
test program. The former checks that the exit code is one of the numbers specified in
the directive; the other makes the test framework ignore the exit code.

{\opp} simulations exit with zero if the simulation terminated
without an error, and some >0 code if a runtime error occurred. Normally,
a nonzero exit code makes the test fail. However, if the expected outcome
is a runtime error (e.g. for some negative test cases), one can use either
\ftest{\%exitcode} to express that, or specify \ftest{\%ignore-exitcode}
and test for the presence of the correct error message in the output.

% FIXME a %postprocess-command nem igy muxik!


\subsubsection{\%file-exists, \%file-not-exists}
\label{sec:testing:opptest:file-exists}

Syntax:

\begin{filelisting}
%file-exists: <filename>
\end{filelisting}

\begin{filelisting}
%file-not-exists: <filename>
\end{filelisting}

These directives test for the presence or absence of a certain file in
the test directory.

\subsection{Extra Processing Steps}
\label{sec:testing:opptest:extra-processing-steps}

\subsubsection{\%env, \%extraargs, \%testprog}
\label{sec:testing:opptest:env-extraargs-testprog}

Syntax:

\begin{filelisting}
%env: <environment-variable-name>=<value>
\end{filelisting}

\begin{filelisting}
%extraargs: <argument-list>
\end{filelisting}

\begin{filelisting}
%testprog: <executable>
\end{filelisting}

The \ftest{\%env} directive lets one set an environment variable that will
be defined when the test program and the potential pre- and post-processing
commands run. There can be multiple \ftest{\%env} directives in the test
file.

\ftest{\%extraargs} lets one add extra command-line arguments to the
test program (usually the simulation) when it is run.

The \ftest{\%testprog} directive lets one replace the test program.
\ftest{\%testprog} also slightly alters the arguments the test program is
run with. Normally, the test program is launched with the following command
line:

\begin{filelisting}
$ <default-testprog> -u Cmdenv <test-extraargs> <global-extraargs> <inifiles>
\end{filelisting}

When \ftest{\%testprog} is present, it becomes the following:

\begin{filelisting}
$ <custom-testprog> <test-extraargs> <global-extraargs>
\end{filelisting}

That is, \fopt{-u Cmdenv} and \ttt{<inifilenames>} are removed; this allows one
to invoke programs that do not require or understand them, and puts the test author
in complete command of the arguments list.

Note that \ftest{\%extraargs} and \ftest{\%testprog} have an equivalent
command-line option in \fprog{opp\_test}. (In the text above,
\ttt{<global-extraargs>} stands for extra args specified to
\fprog{opp\_test}.)  \ftest{\%env} doesn't need an option in
\fprog{opp\_test}, because the test program inherits the environment
variables from \fprog{opp\_test}, so one can just set them in the control
script, or in the shell one runs the tests from.


\subsubsection{\%prerun-command, \%postrun-command}
\label{sec:testing:opptest:prerun-postrun-commands}

Syntax:

\begin{filelisting}
%prerun-command: <command>
\end{filelisting}

\begin{filelisting}
%postrun-command: <command>
\end{filelisting}

These directives let one run extra commands before/after running the test
program (i.e. the simulation). There can be multiple pre- and post-run
commands. The post-run command is useful when the test outcome cannot be determined
by simple text matching, but requires statistical evaluation or other processing.

If the command returns a nonzero exit code, the test framework will assume that
it is due to a technical problem (as opposed to test failure), and count the
test as \textit{error}. To make the test fail, let the command write a
file, and match the file's contents using \ftest{\%contains} \& co.

If the post-processing command is a short script, it is practical
to add it into the \ttt{.test} file via the \ftest{\%file} directive,
and invoke it via its interpreter. For example:

\begin{filelisting}
%postrun-command: python test.py
%file: test.py
<Python script>
\end{filelisting}

Or:

\begin{filelisting}
%postrun-command: R CMD BATCH test.R
%file: test.R
<R script>
\end{filelisting}

If the script is very large or shared among several tests, it is more practical
to place it into a separate file. The test command can find the script e.g.
by relative path, or by referring to an environment variable that contains
its location or full path.


\subsection{Error}
\label{sec:testing:opptest:error}

A test case is considered to be in \textit{error} if the test program cannot be executed
at all, the output cannot be read, or some other technical problem occurred.

\subsection{Expected Failure}
\label{sec:testing:opptest:expected-failure}

\ftest{\%expected-failure} can be used in the test file to force a test case
to ignore a failure. If a test case marked with \ttt{\%expected-failure} fails,
it will be counted as \textit{expectfail} instead of \textit{fail}.
\fprog{opp\_test} will return successfully if no test cases reported \textit{fail}
or \textit{error} results.

Syntax:
\begin{filelisting}
%expected-failure: <single-line-reason-for-allowing-a-failure>
\end{filelisting}

\subsection{Skipped}
\label{sec:testing:opptest:skipped}

A test case can be skipped if the current system configuration does not allow 
its execution (e.g. certain optional features are not present). Skipping is done
by printing \ttt{\#SKIPPED} or \ttt{\#SKIPPED:some-explanation} on the
standard output, at the beginning of the line.

\subsection{opp\_test Synopsys}
\label{sec:testing:opptest:synopsys}

Little has been said so far what \fprog{opp\_test} actually does, or how it
is meant to be run. \fprog{opp\_test} has two modes: file generation and
test running. When running a test suite, \fprog{opp\_test} is actually run
twice, once in file generation mode, then in test running mode.

File generation mode has the syntax \ttt{opp\_test gen \textit{<options>
<testfiles>}}. For example:

\begin{filelisting}
$ opp_test gen *.test
\end{filelisting}

This command will extract C++ and NED files, ini files, etc., from
the \ttt{.test} files into separate files. All files will be created
in a work directory (which defaults to \ttt{./work/}),
and each test will have its own subdirectory under \ttt{./work/}.

The second mode, test running, is invoked as \ttt{opp\_test run \textit{<options>
<testfiles>}}. For example:

\begin{filelisting}
$ opp_test run *.test
\end{filelisting}

In this mode, \fprog{opp\_test} will run the simulations, check the
results, and report the number of passes and failures. The way of invoking
simulations (which executable to run, the list of command-line arguments to
pass, etc.) can be specified to \fprog{opp\_test} via command-line options.

\begin{note}
Run \fprog{opp\_test} in your {\opp} installation to get the exact list of
command-line options.
\end{note}

The simulation needs to have been built from source before \ttt{opp\_test
run} can be issued. Usually one would employ a command similar to

\begin{filelisting}
$ cd work; opp_makemake --deep; make
\end{filelisting}

to achieve that.

\subsection{Writing the Control Script}
\label{sec:testing:opptest:writing-control-script}

Usually one writes a control script to automate the two invocations of \fprog{opp\_test}
and the build of the simulation model between them.

A basic variant would look like this:

\begin{filelisting}
#! /bin/sh
opp_test gen -v *.test || exit 1
(cd work; opp_makemake -f --deep; make) || exit 1
opp_test run -v *.test
\end{filelisting}

For any practical use, the test suite needs to refer to the codebase being
tested. This means that the codebase must be added to the include path,
must be linked with, and the NED files must be added to the NED path. The
first two can be achieved by the appropriate parameterization of
\fprog{opp\_makemake}; the last one can be done by setting and exporting
the \ttt{NEDPATH} environment variable in the control script.

For inspiration, check out \ttt{runtest} in the \ttt{omnetpp/test/core}
directory, and a similar script used in the INET Framework.

\bigskip
\begin{center}
* * *
\end{center}
\bigskip

Further sections describe how one can implement various types of tests in
{\opp}.

\section{Smoke Tests}
\label{sec:testing:smoke-tests}

Smoke tests are a tool for very basic verification and regression testing.
Basically, the simulation is run for a while, and it must not crash or stop
with a runtime error. Naturally, smoke test provide very low confidence in
the model, but in turn they are very easy to implement.

Automation is important. The INET Framework contains a script that runs all
or selected simulations defined in a CSV file (with columns like the working
directory and the command to run), and reports the results. The script can
be easily adapted to other models or model frameworks.


\section{Fingerprint Tests}
\label{sec:testing:fingerprint-tests}

Fingerprint tests are a low-cost but effective tool for regression testing
of simulation models. A fingerprint is a hash computed from various properties
of simulation events, messages and statistics. The hash value is continuously
updated as the simulation executes, and thus, the final fingerprint value is
a characteristic of the simulation's trajectory. For regression testing, one
needs to compare the computed fingerprints to that from a reference run --
if they differ, the simulation trajectory has changed. In general, fingerprint
tests are very useful for ensuring that a change (some refactoring, a bugfix,
or a new feature) didn't break the simulation.

\subsection{Fingerprint Computation}
\label{sec:testing:fingerprint-computation}

Technically, providing a \fconfig{fingerprint} option in the config file or
on the command line (\ttt{-\-fingerprint=...}) will turn on fingerprint
computation in the {\opp} simulation kernel. When the simulation terminates,
{\opp} compares the computed fingerprints with the provided ones, and
if they differ, an error is generated.

\subsubsection{Ingredients}
\label{sec:testing:fingerprint-ingredients}

The fingerprint computation algorithm allows controlling what is included
in the hash value. Changing the \textit{ingredients} allows one to make the
fingerprint sensitive for certain changes while keeping it immune to
others.

The ingredients of a fingerprint are usually indicated after a \ttt{/} sign
following the hexadecimal hash value. Each ingredient is identified with a
letter. For example, \tbf{t} stands for simulation time. Thus, the following
\ffilename{omnetpp.ini} line

\begin{inifile}
fingerprint = 53de-64a7/tplx
\end{inifile}

means that a fingerprint needs to be computed with the simulation time, the
module full path, received packet's bit length and the extra data included
for each event, and the result should be \ttt{53de-64a7}.

The full list of fingerprint ingredients:

%% Latex formatting: compact two-column, use letter as bullet, using multicols and [noitemsep]
\begin{multicols}{2}
\begin{itemize}[noitemsep]
  \item[\tbf{e}]: event number
  \item[\tbf{t}]: simulation time
  \item[\tbf{n}]: message/event full name
  \item[\tbf{c}]: message/event class name
  \item[\tbf{k}]: message kind
  \item[\tbf{l}]: message (packet) bit length
  \item[\tbf{o}]: message control info class name
  \item[\tbf{d}]: message data
  \item[\tbf{i}]: module id
  \item[\tbf{m}]: module full name (name with index)
  \item[\tbf{p}]: module full path (hierarchical name)
  \item[\tbf{a}]: module class name
  \item[\tbf{r}]: random numbers drawn
  \item[\tbf{s}]: scalar results
  \item[\tbf{z}]: statistic results (histogram, etc.)
  \item[\tbf{v}]: vector results
  \item[\tbf{x}]: extra data added programmatically
  \item[\tbf{y}]: display strings
  \item[\tbf{f}]: canvas figures
  \item[\tbf{0}]: clean hasher
\end{itemize}
\end{multicols}

Ingredients may also be specified with the
\fconfig{fingerprint-ingredients} configuration option. However, that is
rarely necessary, because the ingredients list included in the fingerprints
take precedence, and are also more convenient to use.

\subsubsection{Multiple Fingerprints, Alternative Values}
\label{sec:testing:multiple-fingerprints-alternative-values}

It is possible to specify more than one fingerprint, separated by
\textit{commas}, each with different ingredients. This will cause {\opp} to
compute multiple fingerprints, and all of them must match for the test to
pass. An example:

\begin{inifile}
fingerprint = 53de-64a7/tplx, 9a3f-7ed2/szv
\end{inifile}

Occasionally, the same simulation gives a different fingerprint when run on
a different processor architecture or platform. This is due to subtle
differences in floating point arithmetic across platforms.\footnote{There
are differences between the floating point operations of AMD and Intel
CPUs. Running under a processor emulator like \fprog{valgrind} may also
produce a different fingerprint. This is normal. Hint: see gcc options
\fopt{-mfpmath=sse -msse2}.} Acknowledging this fact, {\opp} lets one list
several values for a fingerprint, separated by \textit{spaces}, and will
accept whichever is produced by the simulation. The following example lists
two alternative values for both fingerprints.

\begin{inifile}
fingerprint = 53de-64a7/tplx 63dc-ff21/tplx, 9a3f-7ed2/szv da39-91fc/szv
\end{inifile}

Note that fingerprint computation has been changed and significantly
extended in {\opp} version 5.0.\footnote{The old ({\opp} 4.x) fingerprint
was computed from the module ID and simulation time of each event. To
reproduce a 4.x fingerprint on {\opp} 5.0 or later, compile {\opp} and the
model with \ttt{USE\_OMN\-ETPP4x\_FINGER\-PRINTS} defined. Simply setting the
ingredients to \tbf{ti} is not enough because of additional, subtle changes
in the simulation kernel.}


\subsubsection{Further Filtering}
\label{sec:testing:fingerprint-further-filtering}

It is also possible to filter which modules, statistics, etc. are included
in the fingerprints. The \fconfig{fingerprint-events},
\fconfig{fingerprint-modules}, and \fconfig{fingerprint-results} options
filter by events, modules, and statistical results, respectively. These
options take wildcard expressions that are matched against the
corresponding object before including its property in the fingerprint.
These filters are mainly useful to limit fingerprint computation to certain
parts of the simulation.

\subsubsection{Programmatic Access}
\label{sec:testing:fingerprint-programmatic-access}

\cclass{cFingerprintCalculator} is the class responsible for fingerprint computation.
The current fingerprint computation object can be retrieved from
\cclass{cSimulation}, using the \ffunc{getFingerprintCalculator()} member function.
This method will return \ttt{nullptr} if fingerprint computation is turned
off for the current simulation run.

To contribute data to the fingerprint, \cclass{cFingerprintCalculator} has several
\ffunc{addExtraData()} methods for various data types (string, \ttt{long},
\ttt{double}, byte array, etc.)

An example (note that we check the pointer for \ttt{nullptr} to decide
whether a fingerprint is being computed):

\begin{cpp}
cFingerprintCalculator *fingerprint = getSimulation()->getFingerprintCalculator();
if (fingerprint) {
    fingerprint->addExtraData(retryCount);
    fingerprint->addExtraData(rttEstimate);
}
\end{cpp}

Data added using \ffunc{addExtraData()} will only be counted in the
fingerprint if the list of fingerprint ingredients contains \tbf{x}
(otherwise \ffunc{addExtraData()} does nothing).

\subsection{Fingerprint Tests}
\label{sec:testing:creating-fingerprint-tests}

The INET Framework contains a script for automated fingerprint tests as
well. The script runs all or selected simulations defined in a CSV file
(with columns like the working directory, the command to run, the
simulation time limit, and the expected fingerprints), and reports the
results. The script is extensively used during INET Framework development
to detect regressions, and can be easily adapted to other models or model
frameworks.

Exerpt from a CSV file that prescribes fingerprint tests to run:

\begin{filelisting}
examples/aodv/, ./run -f omnetpp.ini -c Static,   50s,  4c29-95ef/tplx
examples/aodv/, ./run -f omnetpp.ini -c Dynamic,  60s,  8915-f239/tplx
examples/dhcp/, ./run -f omnetpp.ini -c Wired,    800s, e88f-fee0/tplx
examples/dhcp/, ./run -f omnetpp.ini -c Wireless, 500s, faa5-4111/tplx
\end{filelisting}


\section{Unit Tests}
\label{sec:testing:unit-tests}

If a simulation models contains units of code (classes, functions) smaller
than a module, they are candidates for unit testing. For a network simulation
model, examples of such classes are network addresses, fragmentation reassembly
buffers, queues, various caches and tables, serializers and deserializers,
checksum computation, etc.

Unit tests can be implemented as \ttt{.test} files using the \fprog{opp\_test}
tool (the \ftest{\%activity} directive is especially useful here), or
with potentially any other C++ unit testing framework.

When using \ttt{.test} files, the \textit{build} part of the control script
needs to be set up so that it adds the tested library's source folder(s)
to the include path, and also links the library to the test code.

% TODO explain more, with concrete example for the control script?


\section{Module Tests}
\label{sec:testing:module-tests}

{\opp} modules are not as easy to unit test as standalone classes, because
they typically assume a more complex environment, and, especially modules
that implement network protocols, participate in more complex interactions
than the latter.

To test a module in isolation, one needs to place it into a simulation
where the module's normal operation environment (i.e. other modules it
normally communicates with) are replaced by mock objects. Mock objects are
responsible for providing stimuli for the module under test, and (partly)
for checking the response.

Module tests may be implemented in \ttt{.test} files using the \fprog{opp\_test}
tool. A \ttt{.test} file allows one to place the test description, the test setup
and the expected output into a single, compact file, while large files or files shared
among several tests may be factored out and only referenced by \ttt{.test} files.


\section{Statistical Tests}
\label{sec:testing:statistical-tests}

Statistical tests are those where the test outcome is decided on
some statistical property or properties of the simulation results.

Statistical tests may be useful as validation as well as regression testing.

\subsection{Validation Tests}
\label{sec:testing:validation-tests}

Validation tests aim to verify that simulation results correspond to some
reference values, ideally to those obtained from the real system. In
practice, reference values may come from physical measurements, theoretical
values, or another simulator's results.

\subsection{Statistical Regression Tests}
\label{sec:testing:statistical-regression-tests}

After a refactoring that changes the simulation trajectory (e.g. after
eliminating or introducing extra events, or changes in RNG usage), there
may be no other way to do regression testing than checking that the model
produces \textit{statistically} the same results as before.

For statististical regression tests, one needs to perform several
simulation runs with the same configuration but different RNG seeds, and
verify that the results are from the same distributions as before. One can
use \textit{Student's t-test} (for mean) and the \textit{F-test} (for
variance) to check that the ``before'' and the ``after'' sets of results
are from the same distribution.

\subsection{Implementation}
\label{sec:testing:statistical-tests-implementation}

Statistical software like \textit{GNU R} is extremely useful for these
tests.

Statistical tests may also be implemented in \ttt{.test} files. To let the
tool run several simulations within one test, one may use
\ftest{\%extraargs} to pass the \fopt{-r <runs>} option to Cmdenv;
alternatively, one may use \ftest{\%testprog} to have the test tool run
\fprog{opp\_runall} instead of the normal simulation program. For doing the
statistical computations, one may use \ftest{\%postrun-command} to run an R
script. The R script may rely on the \ttt{omnetpp} R package for reading
the result files.

The INET Framework contains statistical tests where one can look for
inspiration.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "usman"
%%% End:

